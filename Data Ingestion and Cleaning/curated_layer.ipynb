{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea224211-f115-4ac2-93e5-e33c37604dfd",
   "metadata": {},
   "source": [
    "`author:` Prashant Prasad Kanth</br>\n",
    "`date:` 25/09/2022 'MM/DD/YYY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75026bd3-ca03-445b-b30e-4991c87b2260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, length, lit\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60404a8c-7f91-4e61-9a6e-7d02aaa1c986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS wdi_curated\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS eea_curated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5371687-afb0-4c9e-82af-69161fa9e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"DROP DATABASE wdi_curated\")\n",
    "# spark.sql(\"DROP DATABASE eea_curated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c65e2d-7e75-44e1-8261-69293fa2ee67",
   "metadata": {},
   "source": [
    "### World Development Indicators data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01a4d344-7e0d-4e84-bb98-612f51aaae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_data = ['datalake/raw/world_development_indicators/date=20220922/WDIData.csv',\n",
    "            'datalake/raw/world_development_indicators/date=20220922/WDICountry.csv',\n",
    "            'datalake/raw/world_development_indicators/date=20220922/WDISeries.csv']\n",
    "wdi_write_path = ['datalake/curated/wdi/data', 'datalake/curated/wdi/country', 'datalake/curated/wdi/series']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e812e043-99a8-4dfa-a118-320fd09d8e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[info_name: string, info_value: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DESC DATABASE extended eea_curated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4fa877ae-16af-497d-86c3-f181a05d3c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------WDIData.csv----------------------------------------\n",
      "Total rows: 383572\n",
      "Rows after dropping null records: 383572\n",
      "Rows after dropping duplicate records: 383572\n",
      "Number of records that have country code with a size other than 3: 0\n",
      "Rows retained after cleaning: 383572\n",
      "Data written in Parquet format at: datalake/curated/wdi/data\n",
      "----------------------------------------WDICountry.csv----------------------------------------\n",
      "Total rows: 278\n",
      "Rows after dropping null records: 278\n",
      "Rows after dropping duplicate records: 278\n",
      "Number of records that have country code with a size other than 3: 13\n",
      "Rows after dropping records with len(country_code) !=3: 265\n",
      "Rows retained after cleaning: 265\n",
      "Data written in Parquet format at: datalake/curated/wdi/country\n",
      "----------------------------------------WDISeries.csv----------------------------------------\n",
      "Total rows: 4282\n",
      "Rows after dropping null records: 4282\n",
      "Rows after dropping duplicate records: 2323\n",
      "Number of records that contain a space character in Series_Code column: 854\n",
      "Rows after dropping records with a space in series_code column: 1469\n",
      "Rows retained after cleaning: 1469\n",
      "Data written in Parquet format at: datalake/curated/wdi/series\n"
     ]
    }
   ],
   "source": [
    "for readpath, writepath in zip(wdi_data, wdi_write_path):\n",
    "    filename = readpath.split('/')[-1]\n",
    "    print(f\"{'--'*20}{filename}{'--'*20}\")\n",
    "    \n",
    "    # read files into dataframe\n",
    "    df = spark.read.options(header=True, inferSchema=True).csv(readpath)\n",
    "    print(f\"Total rows: {df.count()}\")\n",
    "    \n",
    "    # Replace spaces in column names with underscores (“_”) for all DataFrames\n",
    "    column_names = [col_name.replace(' ','_') for col_name in df.columns]\n",
    "    df = df.toDF(*column_names)\n",
    "    # null column read with null header\n",
    "    if filename == \"WDIData.csv\":\n",
    "        df = df.drop(\"_c66\")\n",
    "    \n",
    "    # Drop records that only consist of null values (records with null values on all columns)\n",
    "    df = df.na.drop(\"all\")\n",
    "    print(f\"Rows after dropping null records: {df.count()}\")\n",
    "    \n",
    "    # Drop duplicate records\n",
    "    df = df.dropDuplicates()\n",
    "    print(f\"Rows after dropping duplicate records: {df.count()}\")\n",
    "    \n",
    "    # For the WDICountry.csv and WDIData.csv files, drop all records that have a country code\n",
    "    # (column: Country_Code) with a size other than three\n",
    "    if filename in ['WDICountry.csv', 'WDIData.csv']:\n",
    "        cnt = df.filter(length(col('Country_Code')) != 3).count()\n",
    "        print(f\"Number of records that have country code with a size other than 3: {cnt}\")\n",
    "        if cnt > 0:\n",
    "            df = df.filter(length(col('Country_Code')) == 3)\n",
    "            print(f\"Rows after dropping records with len(country_code) !=3: {df.count()}\")\n",
    "    \n",
    "    # For WDISeries.csv, drop all records that contain a space character (\" \") in the Series_Code column\n",
    "    if filename == 'WDISeries.csv':\n",
    "        cnt = df.filter(col('Series_Code').contains(\" \")).count()\n",
    "        print(f\"Number of records that contain a space character in Series_Code column: {cnt}\")\n",
    "        if cnt > 0:\n",
    "            df = df.filter(~col('Series_Code').contains(\" \"))\n",
    "            print(f\"Rows after dropping records with a space in series_code column: {df.count()}\")\n",
    "    print(f\"Rows retained after cleaning: {df.count()}\")\n",
    "    \n",
    "    # Write data to curated layer\n",
    "    current_time = datetime.now()\n",
    "    df.withColumn(\"year\", lit(current_time.year))\\\n",
    "    .withColumn(\"month\", lit(current_time.month))\\\n",
    "    .withColumn(\"day\", lit(current_time.day))\\\n",
    "    .write.partitionBy(\"year\",\"month\",\"day\").parquet(writepath)\n",
    "    print(\"Data written in Parquet format at: {}\".format(writepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a73c7f6-e6d8-4f80-9de7-d0557da474ca",
   "metadata": {},
   "source": [
    "### CO2 emissions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3f74e0-bf4a-45a9-8f73-3ef2879e1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2emission_data_path = 'datalake/raw/co2_passenger_cars_emissions'\n",
    "co2emission_write_path = 'datalake/curated/co2_emissions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a24f0c-6c15-444b-a776-306a31102314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datalake/raw/co2_passenger_cars_emissions/year=2019/co2_emissions_passenger_cars_2019.json', 'datalake/raw/co2_passenger_cars_emissions/year=2017/co2_emissions_passenger_cars_2017.json', 'datalake/raw/co2_passenger_cars_emissions/year=2018/co2_emissions_passenger_cars_2018.json']\n"
     ]
    }
   ],
   "source": [
    "co2emission_data = []\n",
    "for year in os.listdir(co2emission_data_path):\n",
    "    year_data = os.path.join(co2emission_data_path, year)\n",
    "    for file in os.listdir(year_data):\n",
    "        co2emission_data.append(os.path.join(year_data, file))\n",
    "print(co2emission_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8259f1c7-869a-4f59-b17c-3ef5b09c0f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Processing co2emissions data--------------------\n",
      "Total rows: 300000\n",
      "Rows after dropping null records: 300000\n",
      "Rows after dropping duplicate records: 300000\n",
      "Rows after processing MS column: 299996\n",
      "Rows retained after cleaning: 299996\n",
      "Data written in Parquet format at: datalake/curated/co2_emissions\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'--'*10}Processing co2emissions data{'--'*10}\")\n",
    "co2_cars_emissions = spark.read.json(co2emission_data)\n",
    "print(f\"Total rows: {co2_cars_emissions.count()}\")\n",
    "\n",
    "# Replace spaces in column names with underscores (“_”) for all DataFrames\n",
    "column_names = [col_name.replace(' ','_') for col_name in co2_cars_emissions.columns]\n",
    "# '(', ')' and '/' errors out when saving as parquet, with invalid characters, replacing them\n",
    "column_names = [col_name.replace('(','').replace(')','').replace('/','per') for col_name in column_names]\n",
    "co2_cars_emissions = co2_cars_emissions.toDF(*column_names)\n",
    "\n",
    "# Drop records that only consist of null values (records with null values on all columns)\n",
    "co2_cars_emissions = co2_cars_emissions.na.drop(\"all\")\n",
    "print(f\"Rows after dropping null records: {co2_cars_emissions.count()}\")\n",
    "\n",
    "# Drop duplicate records\n",
    "co2_cars_emissions = co2_cars_emissions.dropDuplicates()\n",
    "print(f\"Rows after dropping duplicate records: {co2_cars_emissions.count()}\")\n",
    "\n",
    "# Drop all records that have a member state code size other than two (column: MS) \n",
    "# and that contain any character other than uppercase letters in this column\n",
    "co2_cars_emissions = co2_cars_emissions.filter((length(col('MS'))==2) & (col('MS').rlike(\"^[A-Z]*$\")))\n",
    "print(f\"Rows after processing MS column: {co2_cars_emissions.count()}\")\n",
    "print(f\"Rows retained after cleaning: {co2_cars_emissions.count()}\")\n",
    "\n",
    "# Write data to curated layer\n",
    "co2_cars_emissions.write.partitionBy(\"year\").parquet(co2emission_write_path)\n",
    "print(\"Data written in Parquet format at: {}\".format(co2emission_write_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed3a62b-2d40-49f5-a651-b11eacf0eb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf02b4-0833-49a7-af24-3350e276e30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 3 in Python 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
